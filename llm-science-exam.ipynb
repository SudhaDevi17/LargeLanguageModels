{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\nfrom string import Template\nfrom pathlib import Path\n\nimport warnings\nwarnings.simplefilter(\"ignore\")\n\nimport torch\nfrom transformers import T5Tokenizer, T5ForConditionalGeneration\n\ndata_path = Path('/kaggle/input/kaggle-llm-science-exam')","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## We'll use `FLAN-T5-base` from Kaggle's Model Hub\n\nYou'll probably want to turn on the GPU option for the notebook! (Remember though, since this is a Code competition, you'll need to set Internet to Off for Notebook submissions to the competition.)","metadata":{}},{"cell_type":"code","source":"llm = '/kaggle/input/flan-t5/pytorch/base/2'\n\n\ndevice = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\nmodel = T5ForConditionalGeneration.from_pretrained(llm).to(device)\ntokenizer = T5Tokenizer.from_pretrained(llm)","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2023-09-20T17:21:03.636818Z","iopub.execute_input":"2023-09-20T17:21:03.637371Z","iopub.status.idle":"2023-09-20T17:21:26.064750Z","shell.execute_reply.started":"2023-09-20T17:21:03.637295Z","shell.execute_reply":"2023-09-20T17:21:26.063503Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tokenizer","metadata":{"execution":{"iopub.status.busy":"2023-09-20T17:43:05.195871Z","iopub.execute_input":"2023-09-20T17:43:05.196254Z","iopub.status.idle":"2023-09-20T17:43:05.204802Z","shell.execute_reply.started":"2023-09-20T17:43:05.196222Z","shell.execute_reply":"2023-09-20T17:43:05.203407Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The data is formatted as follows. For each `prompt` (e.g., the question) there are five possible answers labeled `[A-E]`. Only one of the answers is correct.","metadata":{}},{"cell_type":"code","source":"test = pd.read_csv(data_path / 'test.csv', index_col='id')\ntest.head()","metadata":{"execution":{"iopub.status.busy":"2023-09-20T17:21:26.068149Z","iopub.execute_input":"2023-09-20T17:21:26.069455Z","iopub.status.idle":"2023-09-20T17:21:26.114178Z","shell.execute_reply.started":"2023-09-20T17:21:26.069413Z","shell.execute_reply":"2023-09-20T17:21:26.113131Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Creating a preamble template\n\nHow you format your prompt to input to the LLM can make a big difference in the output you get. Here, we try to instruct the LLM to rank all of the options from most likely to least likely.","metadata":{}},{"cell_type":"code","source":"preamble = \\\n    'Answer the following question by outputting the letters A, B, C, D, and E '\\\n    'in order of the most likely to least likely to be correct option. The predictions must be among the option A, B , C, D and E only'\n\ntemplate = Template('$preamble\\n\\n$prompt\\n\\nA) $a\\nB) $b\\nC) $c\\nD) $d\\nE) $e')","metadata":{"execution":{"iopub.status.busy":"2023-09-20T17:47:12.184050Z","iopub.execute_input":"2023-09-20T17:47:12.184475Z","iopub.status.idle":"2023-09-20T17:47:12.189950Z","shell.execute_reply.started":"2023-09-20T17:47:12.184442Z","shell.execute_reply":"2023-09-20T17:47:12.188466Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"template","metadata":{"execution":{"iopub.status.busy":"2023-09-20T17:47:12.791846Z","iopub.execute_input":"2023-09-20T17:47:12.792258Z","iopub.status.idle":"2023-09-20T17:47:12.798722Z","shell.execute_reply.started":"2023-09-20T17:47:12.792225Z","shell.execute_reply":"2023-09-20T17:47:12.797446Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test.loc[0, 'prompt']","metadata":{"execution":{"iopub.status.busy":"2023-09-20T17:47:13.594214Z","iopub.execute_input":"2023-09-20T17:47:13.594649Z","iopub.status.idle":"2023-09-20T17:47:13.601584Z","shell.execute_reply.started":"2023-09-20T17:47:13.594618Z","shell.execute_reply":"2023-09-20T17:47:13.600460Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def format_input(df, idx):\n    \n    prompt = df.loc[idx, 'prompt']\n    a = df.loc[idx, 'A']\n    b = df.loc[idx, 'B']\n    c = df.loc[idx, 'C']\n    d = df.loc[idx, 'D']\n    e = df.loc[idx, 'E']\n\n    input_text = template.substitute(\n        preamble=preamble, prompt=prompt, a=a, b=b, c=c, d=d, e=e)\n    \n    return input_text","metadata":{"execution":{"iopub.status.busy":"2023-09-20T17:47:13.887356Z","iopub.execute_input":"2023-09-20T17:47:13.887954Z","iopub.status.idle":"2023-09-20T17:47:13.895209Z","shell.execute_reply.started":"2023-09-20T17:47:13.887919Z","shell.execute_reply":"2023-09-20T17:47:13.893953Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"This is an example of a formatted question that would be used as input to the LLM.","metadata":{}},{"cell_type":"code","source":"print(format_input(test, 2))","metadata":{"execution":{"iopub.status.busy":"2023-09-20T17:47:16.783181Z","iopub.execute_input":"2023-09-20T17:47:16.783957Z","iopub.status.idle":"2023-09-20T17:47:16.790413Z","shell.execute_reply.started":"2023-09-20T17:47:16.783914Z","shell.execute_reply":"2023-09-20T17:47:16.789252Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"inputs = tokenizer(format_input(test, 0), return_tensors=\"pt\").to(device)\noutputs = model.generate(**inputs)\nanswer = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n\nprint(answer)","metadata":{"execution":{"iopub.status.busy":"2023-09-20T17:47:19.609139Z","iopub.execute_input":"2023-09-20T17:47:19.610087Z","iopub.status.idle":"2023-09-20T17:47:19.686102Z","shell.execute_reply.started":"2023-09-20T17:47:19.610037Z","shell.execute_reply":"2023-09-20T17:47:19.684998Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Post-processing\n\nYou can see from the above that the LLM did not properly follow instructions. You'll need to figure out how to ensure your model provides at least the top three predictions, and have checks and post-processing in place for when they don't (such as in our example!)\n\nThis notebook provides a naive and **very** fragile example of how to do this. You'll want to make something more rubust!","metadata":{}},{"cell_type":"code","source":"def post_process(predictions):\n    valid = set(['A', 'B', 'C', 'D', 'E'])\n    # If there are no valid choices, return something and hope for partial credit\n    if set(predictions).isdisjoint(valid):\n        final_pred = 'A B C D E'\n    else:\n        final_pred = []\n        for prediction in predictions:\n            if prediction in valid:\n                final_pred += prediction\n        # add remaining letters\n        to_add = valid - set(final_pred)\n        final_pred.extend(list(to_add))\n        # put in space-delimited format\n        final_pred = ' '.join(final_pred)\n        \n    return final_pred","metadata":{"execution":{"iopub.status.busy":"2023-09-20T17:47:27.140226Z","iopub.execute_input":"2023-09-20T17:47:27.140648Z","iopub.status.idle":"2023-09-20T17:47:27.147620Z","shell.execute_reply.started":"2023-09-20T17:47:27.140612Z","shell.execute_reply":"2023-09-20T17:47:27.146543Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Making a submission\n\nWe can now make a simple script to make a submission to the competition.","metadata":{}},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission = pd.read_csv(\n    data_path / 'sample_submission.csv', index_col='id')\n\nfor idx in test.index:\n    inputs = tokenizer(format_input(test, idx), return_tensors=\"pt\").to(device)\n    outputs = model.generate(**inputs)\n    answer = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n    submission.loc[idx, 'prediction'] = post_process(answer)","metadata":{"execution":{"iopub.status.busy":"2023-09-20T17:48:26.089784Z","iopub.execute_input":"2023-09-20T17:48:26.090189Z","iopub.status.idle":"2023-09-20T17:48:38.519531Z","shell.execute_reply.started":"2023-09-20T17:48:26.090158Z","shell.execute_reply":"2023-09-20T17:48:38.518421Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"You can include all five possible answers, but only the first three will be counted!","metadata":{}},{"cell_type":"code","source":"submission.head()","metadata":{"execution":{"iopub.status.busy":"2023-09-20T17:48:46.886619Z","iopub.execute_input":"2023-09-20T17:48:46.887016Z","iopub.status.idle":"2023-09-20T17:48:46.899585Z","shell.execute_reply.started":"2023-09-20T17:48:46.886983Z","shell.execute_reply":"2023-09-20T17:48:46.898355Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission.to_csv('submission.csv')","metadata":{"execution":{"iopub.status.busy":"2023-09-20T17:48:56.228518Z","iopub.execute_input":"2023-09-20T17:48:56.228912Z","iopub.status.idle":"2023-09-20T17:48:56.237773Z","shell.execute_reply.started":"2023-09-20T17:48:56.228875Z","shell.execute_reply":"2023-09-20T17:48:56.236333Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}